

@article{Dalmasso2019a,
abstract = {Hurricanes and, more generally, tropical cyclones (TCs) are rare, complex natural phenomena of both scientific and public interest. The importance of understanding TCs in a changing climate has increased as recent TCs have had devastating impacts on human lives and communities. Moreover, good prediction and understanding about the complex nature of TCs can mitigate some of these human and property losses. Though TCs have been studied from many different angles, more work is needed from a statistical approach of providing prediction regions. The current state-of-the-art in TC prediction bands comes from the National Hurricane Center of the National Oceanographic and Atmospheric Administration (NOAA), whose proprietary model provides "cones of uncertainty" for TCs through an analysis of historical forecast errors. The contribution of this paper is twofold. We introduce a new pipeline that encourages transparent and adaptable prediction band development by streamlining cyclone track simulation and prediction band generation. We also provide updates to existing models and novel statistical methodologies in both areas of the pipeline, respectively.},
archivePrefix = {arXiv},
arxivId = {1906.08832},
author = {Dalmasso, Niccol{\`{o}} and Dunn, Robin and LeRoy, Benjamin and Schafer, Chad},
eprint = {1906.08832},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/tropical{\_}cyclones/A Flexible Pipeline for Prediction of Tropical Cyclone Paths.pdf:pdf},
title = {{A Flexible Pipeline for Prediction of Tropical Cyclone Paths}},
url = {http://arxiv.org/abs/1906.08832},
year = {2019}
}
@article{Dalmasso2017,
author = {Dalmasso, Niccol$\backslash$'o and Dunn, Robin and LeRoy, Benjamin and Schafer, Chad},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/icml{\_}papers/A Data-Driven Framework for Tropical Cyclone Path Estimation{\_}reject{\_}JASA.pdf:pdf},
title = {{A Data-Driven Framework for Tropical Cyclone Path Estimation}},
year = {2019}
}
@article{Harko2014,
abstract = {In this paper, the exact analytical solution of the Susceptible-Infected- Recovered (SIR) epidemic model is obtained in a parametric form. By using the exact solution we investigate some explicit models corresponding to fixed values of the parameters, and show that the numerical solution reproduces exactly the analytical solution. We also show that the generalization of the SIR model, including births and deaths, described by a nonlinear system of differential equations, can be reduced to an Abel type equation. The reduction of the complex SIR model with vital dynamics to an Abel type equation can greatly simplify the analysis of its properties. The general solution of the Abel equation is obtained by using a perturbative approach, in a power series form, and it is shown that the general solution of the SIR model with vital dynamics can be represented in an exact parametric form. {\textcopyright} 2014 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1403.2160},
author = {Harko, Tiberiu and Lobo, Francisco S.N. and Mak, M. K.},
doi = {10.1016/j.amc.2014.03.030},
eprint = {1403.2160},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/Exact analytical solutions of the Susceptible-Infected-Recovered (SIR) epidemic model and of the SIR model with equal death and birth rates.pdf:pdf},
issn = {00963003},
journal = {Applied Mathematics and Computation},
keywords = {Abel equation,Exact solution,Susceptible-Infected-Recovered (SIR) epidemic mode},
pages = {184--194},
publisher = {Elsevier Inc.},
title = {{Exact analytical solutions of the Susceptible-Infected-Recovered (SIR) epidemic model and of the SIR model with equal death and birth rates}},
url = {http://dx.doi.org/10.1016/j.amc.2014.03.030},
volume = {236},
year = {2014}
}
@article{Sanche2020a,
abstract = {Severe acute respiratory syndrome coronavirus 2 is the causative agent of the 2019 novel coronavirus disease pandemic. Initial estimates of the early dynamics of the outbreak in Wuhan, China, suggested a doubling time of the number of infected persons of 6-7 days and a basic reproductive number (R0) of 2.2-2.7. We collected extensive individual case reports across China and estimated key epidemiologic parameters, including the incubation period. We then designed 2 mathematical modeling approaches to infer the outbreak dynamics in Wuhan by using high-resolution domestic travel and infection data. Results show that the doubling time early in the epidemic in Wuhan was 2.3-3.3 days. Assuming a serial interval of 6-9 days, we calculated a median R0 value of 5.7 (95{\%} CI 3.8-8.9). We further show that active surveillance, contact tracing, quarantine, and early strong social distancing efforts are needed to stop transmission of the virus.},
author = {Sanche, Steven and Lin, Yen Ting and Xu, Chonggang and Romero-Severson, Ethan and Hengartner, Nick and Ke, Ruian},
doi = {10.3201/eid2607.200282},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/High Contagiousness and Rapid Spread of Severe Acute Respiratory Syndrome Coronavirus 2.pdf:pdf},
issn = {10806059},
journal = {Emerging infectious diseases},
keywords = {2019 novel coronavirus disease,COVID-19,China,SARS-CoV-2,Wuhan,modeling,respiratory infections,severe acute respiratory syndrome coronavirus 2,transmission potential,viruses,zoonoses},
month = {apr},
number = {7},
pmid = {32255761},
publisher = {NLM (Medline)},
title = {{High Contagiousness and Rapid Spread of Severe Acute Respiratory Syndrome Coronavirus 2}},
volume = {26},
year = {2020}
}

@article{Lopez2009,
abstract = {The statistical analysis of functional data is a growing need in many research areas. In particular, a robust methodology is important to study curves, which are the output of many experiments in applied statistics. As a starting point for this robust analysis, we propose, analyze, and apply a new definition of depth for functional observations based on the graphic representation of the curves. Given a collection of functions, it establishes the "centrality" of an observation and provides a natural center-outward ordering of the sample curves. Robust statistics, such as the median function or a trimmed mean function, can be defined from this depth definition. Its finite-dimensional version provides a new depth for multivariate data that is computationally feasible and useful for studying high-dimensional observations. Thus, this new depth is also suitable for complex observations such as microarray data, images, and those arising in some recent marketing and financial studies. Natural properties of these new concepts are established and the uniform consistency of the sample depth is proved. Simulation results show that the corresponding depth based trimmed mean presents better performance than other possible location estimators proposed in the literature for some contaminated models. Data depth can be also used to screen for outliers. The ability of the new notions of depth to detect "shape" outliers is presented. Several real datasets are considered to illustrate this new concept of depth, including applications to microarray observations, weather data, and growth curves. Finally, through this depth, we generalize to functions the Wilcoxon rank sum test. It allows testing whether two groups of curves come from the same population. This functional rank test when applied to children growth curves shows different growth patterns for boys and girls. {\textcopyright} 2009 American Statistical Association.},
annote = {This article presents 2 "band" depth-based approaches for 1d functional data. They claim to show that central bands are robust (like a median) using simulation (I gather the induce different types of pertibuations and show the bands don't really change that much. I'm not sure that's super useful).

Definitions of centrality:

1. (Section 2): 
Defn(a): let B(x1,...,xk) = the band that contains all points (t,r) such that min xi(t) {\textless} y {\textless} max xi(t)
Defn(b): Let (Band depth{\_}j) BD{\_}n{\^{}}(j)(x) = proportion of number of bands with j functions that contain x.
Defn(c): Let (Band depth{\_}J) BD{\_}n,J(x) = sum{\_}{\{}j =1{\}}{\^{}}J BD{\_}n{\^{}}(j)(x) --- **very similar to Simplexical Depth by Liu**.

* Proposed only using J = 3 for computation reasons* 
{\#}{\#} Aside: does this mirror Geenen's triangular inequality?


2. (Section 5: supposed to be better)
basically does the above, but changes BD{\_}n{\^{}}(j)(x) to MBD{\_}n{\^{}}(j)(x), where the later sums the proportion of time a function is contained in each specific band -this is easy to do with functions defined relative to finite steps.

suggests that BD focuses more on similarity of structure of curves whereas the MBD focuses less on this.

Other comments:
(Section 3.1) Claims this depth meets all properties from Zuo {\&} Serfling except affine invariance (and suggests this is not a natural requirement for functional depth)

Another comment: different measures deal with noise different


FUTURE WORK:
1. This could be compared with the distance based depth (Geenen): specific - how do these bands of 3 curves relate to the triangle inequality structure proposed by Greenen? And how would it relative to just 2 curves?
2. Think about global depth and centrality in the lense of robustness? How to think about local depth? Vs density estimates?
3. follow up on Cuevas, Febrero, and Fraiman's ‘Robust Estimation and Classification for Functional Data via Projection-Based Depth Notions' - as it relates to the density vs depth debate},
author = {Lopez-Pintado, Sara and Romo, Juan},
doi = {10.1198/jasa.2009.0108},
file = {:Users/benjaminleroy/Library/Application Support/Mendeley Desktop/Downloaded/Pez-Pintado, Romo - 2009 - On the concept of depth for functional data.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Data depth,Functional data,Rank test for functions,Rank test functions},
mendeley-tags = {Data depth,Functional data,Rank test functions},
month = {jun},
number = {486},
pages = {718--734},
title = {{On the Concept of Depth for Functional Data}},
volume = {104},
year = {2009}
}
@article{Jenness2018,
abstract = {Package EpiModel provides tools for building, simulating, and analyzing mathematical models for the population dynamics of infectious disease transmission in R. Several classes of models are included, but the unique contribution of this software package is a general stochastic framework for modeling the spread of epidemics on networks. EpiModel integrates recent advances in statistical methods for network analysis (temporal exponential random graph models) that allow the epidemic modeling to be grounded in empirical data on contacts that can spread infection. This article provides an overview of both the modeling tools built into EpiModel, designed to facilitate learning for students new to modeling, and the application programming interface for extending package EpiModel, designed to facilitate the exploration of novel research questions for advanced modelers.},
author = {Jenness, Samuel M and Goodreau, Steven M and Morris, Martina},
doi = {10.18637/jss.v084.i08.EpiModel},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/EpiModel.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {R,epidemiology,infectious disease,mathematical model,networks},
number = {84},
title = {{EpiModel: An R Package for Mathematical Modeling of Infectious Disease over Networks}},
year = {2018}
}
@article{Canzani2015,
abstract = {The relevance of modeling epidemics' spread goes beyond the academic. The mathematical understanding of infectious diseases has become an important tool in policy making. Our research interest is modeling of dynamics in crisis situations. This paper explores the extant body of literature of mathematical models in epidemiology, with particular emphasis on theories and methodologies used beyond them. Our goal is to identify core building blocks of models and research patterns to model the dynamics of crisis situations such as epidemics. The wide range of applications of epidemic models to many other disciplines that show biological analogies, makes this paper helpful for many modelers and mathematicians within the broader field of Crisis Management.},
author = {Canzani, Elisa and Lechner, Ulrike},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/Insights from Modeling Epidemics of Infectious Diseases – A Literature Review.pdf:pdf},
isbn = {9788271177881},
journal = {ISCRAM 2015 Conference Proceedings - 12th International Conference on Information Systems for Crisis Response and Management},
keywords = {Crisis modeling,Dynamics,Epidemics modeling,Infectious diseases},
title = {{Insights from modeling epidemics of infectious diseases - A literature review}},
volume = {2015-Janua},
year = {2015}
}
@article{Bongiorno2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.04308v2},
author = {Bongiorno, Enea G and Goia, Aldo and Orientale, Piemonte},
eprint = {arXiv:1501.04308v2},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/depth/Some Insights About the Small Ball Probability Factorization for Hilbert Random Elements.pdf:pdf},
keywords = {hilbert functional data,karhunen,lo,small ball probability},
pages = {1--27},
title = {{Some Insights About the Small Ball Probability Factorization for Hilbert Random Elements arXiv : 1501 . 04308v2 [ math . PR ] 29 Mar 2016}},
year = {2016}
}
@article{King2016,
abstract = {Partially observed Markov process (POMP) models, also known as hidden Markov models or state space models, are ubiquitous tools for time series analysis. The R package pomp provides a very flexible framework for Monte Carlo statistical investigations using nonlinear, non-Gaussian POMP models. A range of modern statistical methods for POMP models have been implemented in this framework including sequential Monte Carlo, iterated filtering, particle Markov chain Monte Carlo, approximate Bayesian computation, maximum synthetic likelihood estimation, nonlinear forecasting, and trajectory matching. In this paper, we demonstrate the application of these methodologies using some simple toy problems. We also illustrate the specification of more complex POMP models, using a nonlinear epidemiological model with a discrete population, seasonality, and extra-demographic stochasticity. We discuss the specification of user-defined models and the development of additional methods within the programming environment provided by pomp.},
archivePrefix = {arXiv},
arxivId = {1509.00503},
author = {King, Aaron A. and Nguyen, Dao and Ionides, Edward L.},
doi = {10.18637/jss.v069.i12},
eprint = {1509.00503},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/pomp.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Hidden markov model,Markov processes,Maximum likelihood,Mechanistic model,Plug-and-play,R,Sequential Monte Carlo,State space model,Stochastic dynamical system,Time series},
number = {12},
pages = {1--43},
title = {{Statistical inference for partially observed markov processes via the R package pomp}},
volume = {69},
year = {2016}
}
@book{Vovk2005,
author = {Vovk, Vladimir and Gammerman, Alex and Shafer, Glenn},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/depth/2005{\_}Book{\_}AlgorithmicLearningInARandomWo.pdf:pdf},
isbn = {0-387-00152-2},
publisher = {Springer Science {\&} Business Media},
title = {{Algorithmic Learning in a Random World}},
year = {2005}
}
@misc{Gillespie1976,
author = {Gillespie, Daniel T},
booktitle = {Journal of Computational Physics},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/A General Method for Numerically Simulating the Stochastic Time Evolution of Coupled Chemical Reactions.pdf:pdf},
pages = {403--434},
title = {{A general method for numerically simulating coupled chemical reactions}},
volume = {22},
year = {1976}
}
@article{Neal2004,
abstract = {A stochastic epidemic model is proposed which incorporates heterogeneity in the spread of a disease through a population. In particular, three factors are considered: the spatial location of an individual's home and the household and school class to which the individual belongs. The model is applied to an extremely informative measles data set and the model is compared with nested models, which incorporate some, but not all, of the aforementioned factors. A reversible jump Markov chain Monte Carlo algorithm is then introduced which assists in selecting the most appropriate model to fit the data. {\textcopyright} Oxford University Press 2004; all rights reserved.},
author = {Neal, Peter J. and Roberts, Gareth O.},
doi = {10.1093/biostatistics/5.2.249},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/Statistical inference and model selection for the 1861 Hagelloch measles epidemic.pdf:pdf},
issn = {14654644},
journal = {Biostatistics},
keywords = {Model choice,Reversible jump MCMC,Stochastic epidemics},
number = {2},
pages = {249--261},
title = {{Statistical inference and model selection for the 1861 Hagelloch measles epidemic}},
volume = {5},
year = {2004}
}
@misc{Fisher2020,
author = {Fisher, Max},
booktitle = {The New York Times},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/Coronavirus R0 Value Explained - The New York Times.pdf:pdf},
title = {{R0, the Messy Metric That May Soon Shape Our Lives, Explained}},
url = {https://www.nytimes.com/2020/04/23/world/europe/coronavirus-R0-explainer.html?referringSource=articleShare},
urldate = {2020-05-02},
year = {2020}
}
@techreport{Aronson2020,
author = {Aronson, Jeffrey K and Brassey, Jon and Mahtani, Kamal R},
booktitle = {Centre for Evidence-Based Medicine, Nuffield Department of Primary Care Health Sciences},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/“When-will-it-be-over{\_}”{\_}-An-introduction-to-viral-reproduction-numbers-1.pdf:pdf},
institution = {University of Oxford},
title = {{“When will it be over?”: An introduction to viral reproduction numbers, R0 and Re}},
url = {https://www.cebm.net/covid-19/when-will-it-be-over-an-introduction-to-viral-reproduction-numbers-r0-and-re/},
year = {2020}
}
@article{Gallagher2018,
abstract = {Agent-based models (ABMs) simulate interactions between autonomous agents in constrained environments over time and are often used for modeling the spread of infectious diseases. ABMs use information about agents and their environments as input, together referred to as a “synthetic ecosystem.” Previous approaches for generating synthetic ecosystems have some limitations: they are not open-source, cannot be adapted to new or updated input data sources, or do not allow for alternative methods for sampling agent characteristics and locations. We introduce a general framework for generating synthetic ecosystems, called “Synthetic Populations and Ecosystems of the World” (SPEW). SPEW lets researchers choose from a variety of sampling methods for agent characteristics and locations and is implemented as an open-source R package. We analyze the accuracy and computational efficiency of SPEW, given different sampling methods for agent characteristics and locations, and provide a suite of statistical and graphical tools to screen our generated ecosystems. SPEW has generated over five billion human agents across approximately 100,000 geographic regions in over 70 countries available online.},
archivePrefix = {arXiv},
arxivId = {1701.02383},
author = {Gallagher, Shannon and Richardson, Lee F. and Ventura, Samuel L. and Eddy, William F.},
doi = {10.1080/10618600.2018.1442342},
eprint = {1701.02383},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/SPEW- Synthetic Populations and Ecosystems of the World.pdf:pdf},
issn = {15372715},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Agent-based models,Disease modeling,Iterative proportional fitting,Parallel computing,R,Synthetic data},
number = {4},
pages = {773--784},
publisher = {Taylor {\&} Francis},
title = {{SPEW: Synthetic Populations and Ecosystems of the World}},
url = {https://doi.org/10.1080/10618600.2018.1442342},
volume = {27},
year = {2018}
}
@article{Agostinelli2011,
abstract = {Local depth is a generalization of ordinary depth able to reveal local features of the probability distribution. Liu's simplicial depth is primarily used, but results for Tukey's halfspace depth are also derived. It is shown that the maximizers of local depth can help to detect the mode(s) of a probability distribution. This work is devoted to the univariate case, but the main definitions are stated in the general multivariate case. Theoretical results and applications are illustrated with several examples. {\textcopyright} 2010 Elsevier B.V.},
annote = {This paper proposes 2 local depth that switches the definitions of the global depths they update in such a ways the the geometry changes are pretty clean.

Local depth definitions:
1. Local simplicial depth (t is tuning): 
ld{\_}s(x, t) = Proportion of simplicials of size p that contain x (p should be at least d + 1 - where d is size of space) *is the extension of this actually check the triangle inequality and see if the max distance is below tau?*

2. Local halfspace depth:
d{\_}hs: halfspace is converted into slabs. then same definition is applied

In 4.1 they have some examples but also discuss some quick ways one might tune tau (specifically using the distribution of |X{\_}1 - X{\_}2| (in 1d space)

- example (22) shows that - with too small a selection of tau, you gets a high number of patial maximum ("local") - think about how to avoid it?

- proposed examining |x{\_}1 - x{\_}2| distribution (For 1d) to select tau.
* could we provide analysis to how tau is "just right"? (min should probably be our $\backslash$delta

- also mentioned a way to test if a point is a mode (this is depend on tau {\&} it seems to have the reverse null hypothesis then as desired) - not sure this is need to be extended -but if there was a way to think about H{\_}t,n as a function of tau we might be able to better capture tau. 

- I don't think tau really needs to go to zero as n-{\textgreater} infinite. Does this match with KDEs?},
author = {Agostinelli, Claudio and Romanazzi, Mario},
doi = {10.1016/j.jspi.2010.08.001},
file = {:Users/benjaminleroy/Library/Application Support/Mendeley Desktop/Downloaded/Agostinelli, Romanazzi - 2011 - Local depth.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Data depth,Halfspace depth,Multimodal distribution,Simplicial depth},
month = {feb},
number = {2},
pages = {817--830},
publisher = {Elsevier B.V.},
title = {{Local depth}},
volume = {141},
year = {2011}
}
@misc{Geenens2017,
abstract = {The concept of depth has proved very important for multivariate and functional data analysis, as it essentially acts as a surrogate for the notion a ranking of observations which is absent in more than one dimension. Yet, naive attempts to extend to the functional context some succesful multivariate depth measures have been seen to lead to absurd results, as they failed to take into acount in their construction purely functional features such as continuity, smoothness and contiguity. In this paper we suggest a natural depth which satisfies all the properties that a valid functional depth measure should fulfill. It is explicitly constructed on a certain distance d between functional objects, which has to be selected by the analyst. This allows the depth measure to really be tailored to the data at hand and to the ultimate goal of the analysis, a very desirable property in functional data analysis given the polymorphic nature of those data. This flexibility is thoroughly illustrated by several real data analyses, including an original one pertaining to the field of Symbolic Data Analysis},
author = {Geenens, Gery and Nieto-Reyes, Alicia},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/depth/On the functional distance-based depth.pdf:pdf},
title = {{On the functional distance-based depth}},
year = {2017}
}
@article{Biggerstaff2016,
abstract = {Background: Early insights into the timing of the start, peak, and intensity of the influenza season could be useful in planning influenza prevention and control activities. To encourage development and innovation in influenza forecasting, the Centers for Disease Control and Prevention (CDC) organized a challenge to predict the 2013-14 Unites States influenza season. Methods: Challenge contestants were asked to forecast the start, peak, and intensity of the 2013-2014 influenza season at the national level and at any or all Health and Human Services (HHS) region level(s). The challenge ran from December 1, 2013-March 27, 2014; contestants were required to submit 9 biweekly forecasts at the national level to be eligible. The selection of the winner was based on expert evaluation of the methodology used to make the prediction and the accuracy of the prediction as judged against the U.S. Outpatient Influenza-like Illness Surveillance Network (ILINet). Results: Nine teams submitted 13 forecasts for all required milestones. The first forecast was due on December 2, 2013; 3/13 forecasts received correctly predicted the start of the influenza season within one week, 1/13 predicted the peak within 1 week, 3/13 predicted the peak ILINet percentage within 1 {\%}, and 4/13 predicted the season duration within 1 week. For the prediction due on December 19, 2013, the number of forecasts that correctly forecasted the peak week increased to 2/13, the peak percentage to 6/13, and the duration of the season to 6/13. As the season progressed, the forecasts became more stable and were closer to the season milestones. Conclusion: Forecasting has become technically feasible, but further efforts are needed to improve forecast accuracy so that policy makers can reliably use these predictions. CDC and challenge contestants plan to build upon the methods developed during this contest to improve the accuracy of influenza forecasts.},
author = {Biggerstaff, Matthew and Alper, David and Dredze, Mark and Fox, Spencer and Fung, Isaac Chun Hai and Hickmann, Kyle S. and Lewis, Bryan and Rosenfeld, Roni and Shaman, Jeffrey and Tsou, Ming Hsiang and Velardi, Paola and Vespignani, Alessandro and Finelli, Lyn and Chandra, Priyadarshini and Kaup, Hemchandra and Krishnan, Ramesh and Madhavan, Satish and Markar, Ashirwad and Pashley, Bryanne and Paul, Michael and Meyers, Lauren Ancel and Eggo, Rosalind and Henderson, Jette and Ramakrishnan, Anurekha and Scott, James and Singh, Bismark and Srinivasan, Ravi and Bakach, Iurii and Hao, Yi and Schaible, Braydon J. and Sexton, Jessica K. and {Del Valle}, Sara Y. and Deshpande, Alina and Fairchild, Geoffrey and Generous, Nicholas and Priedhorsky, Reid and Hickman, Kyle S. and Hyman, James M. and Brooks, Logan and Farrow, David and Hyun, Sangwon and Tibshirani, Ryan J. and Yang, Wan and Allen, Christopher and Aslam, Anosh{\'{e}} and Nagel, Anna and Stilo, Giovanni and Basagni, Stefano and Zhang, Qian and Perra, Nicola and Chakraborty, Prithwish and Butler, Patrick and Khadivi, Pejman and Ramakrishnan, Naren and Chen, Jiangzhuo and Barrett, Chris and Bisset, Keith and Eubank, Stephen and {Anil Kumar}, V. S. and Laskowski, Kathy and Lum, Kristian and Marathe, Madhav and Aman, Susan and Brownstein, John S. and Goldstein, Ed and Lipsitch, Marc and Mekaru, Sumiko R. and Nsoesie, Elaine O. and Gesualdo, Francesco and Tozzi, Alberto E. and Broniatowski, David and Karspeck, Alicia and Tse, Zion Tsz Ho and Ying, Yuchen and Gambhir, Manoj and Scarpino, Sam},
doi = {10.1186/s12879-016-1669-x},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/Results from the centers for disease control and prevention's predict the 2013–2014 Influenza Season Challenge.pdf:pdf},
issn = {14712334},
journal = {BMC Infectious Diseases},
keywords = {Forecasting,Influenza,Modeling,Prediction},
number = {1},
pages = {1--10},
publisher = {BMC Infectious Diseases},
title = {{Results from the centers for disease control and prevention's predict the 2013-2014 Influenza Season Challenge}},
url = {http://dx.doi.org/10.1186/s12879-016-1669-x},
volume = {16},
year = {2016}
}
@article{Gallagher2020,
abstract = {For nearly a century, the initial reproduction number (R0) has been used as a one number summary to compare outbreaks of infectious disease, yet there is no `standard' estimator for R0. Difficulties in estimating R0 arise both from how a disease transmits through a population as well as from differences in statistical estimation method. We describe eight methods used to estimate R0 and provide a thorough simulation study of how these estimates change in the presence of different disease parameters. As motivation, we analyze the 2009 outbreak of the H1N1 pandemic influenza in the USA and compare the results from our eight methods to a previous study. We discuss the most important aspects from our results which effect the estimation of R0, which include the population size, time period used, and the initial percent of infectious individuals. Additionally, we discuss how pre-processing incidence counts may effect estimates of R0. Finally, we provide guidelines for estimating point estimates and confidence intervals to create reliable, comparable estimates of R0.},
archivePrefix = {arXiv},
arxivId = {2003.10442},
author = {Gallagher, Shannon and Chang, Andersen and Eddy, William F},
eprint = {2003.10442},
file = {:Users/benjaminleroy/Library/Application Support/Mendeley Desktop/Downloaded/Gallagher, Chang, Eddy - 2020 - EXPLORING THE NUANCES OF R 0 EIGHT ESTIMATES AND APPLICATION TO 2009 PANDEMIC INFLUENZA A PREPRINT.pdf:pdf},
title = {{Exploring the nuances of R0: Eight estimates and application to 2009 pandemic influenza}},
url = {http://arxiv.org/abs/2003.10442},
year = {2020}
}
