Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@misc{Geenens2017,
abstract = {The concept of depth has proved very important for multivariate and functional data analysis, as it essentially acts as a surrogate for the notion a ranking of observations which is absent in more than one dimension. Yet, naive attempts to extend to the functional context some succesful multivariate depth measures have been seen to lead to absurd results, as they failed to take into acount in their construction purely functional features such as continuity, smoothness and contiguity. In this paper we suggest a natural depth which satisfies all the properties that a valid functional depth measure should fulfill. It is explicitly constructed on a certain distance d between functional objects, which has to be selected by the analyst. This allows the depth measure to really be tailored to the data at hand and to the ultimate goal of the analysis, a very desirable property in functional data analysis given the polymorphic nature of those data. This flexibility is thoroughly illustrated by several real data analyses, including an original one pertaining to the field of Symbolic Data Analysis},
author = {Geenens, Gery and Nieto-Reyes, Alicia},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/depth/On the functional distance-based depth.pdf:pdf},
title = {{On the functional distance-based depth}},
year = {2017}
}
@article{Lopez2009,
abstract = {The statistical analysis of functional data is a growing need in many research areas. In particular, a robust methodology is important to study curves, which are the output of many experiments in applied statistics. As a starting point for this robust analysis, we propose, analyze, and apply a new definition of depth for functional observations based on the graphic representation of the curves. Given a collection of functions, it establishes the "centrality" of an observation and provides a natural center-outward ordering of the sample curves. Robust statistics, such as the median function or a trimmed mean function, can be defined from this depth definition. Its finite-dimensional version provides a new depth for multivariate data that is computationally feasible and useful for studying high-dimensional observations. Thus, this new depth is also suitable for complex observations such as microarray data, images, and those arising in some recent marketing and financial studies. Natural properties of these new concepts are established and the uniform consistency of the sample depth is proved. Simulation results show that the corresponding depth based trimmed mean presents better performance than other possible location estimators proposed in the literature for some contaminated models. Data depth can be also used to screen for outliers. The ability of the new notions of depth to detect "shape" outliers is presented. Several real datasets are considered to illustrate this new concept of depth, including applications to microarray observations, weather data, and growth curves. Finally, through this depth, we generalize to functions the Wilcoxon rank sum test. It allows testing whether two groups of curves come from the same population. This functional rank test when applied to children growth curves shows different growth patterns for boys and girls. {\textcopyright} 2009 American Statistical Association.},
annote = {This article presents 2 "band" depth-based approaches for 1d functional data. They claim to show that central bands are robust (like a median) using simulation (I gather the induce different types of pertibuations and show the bands don't really change that much. I'm not sure that's super useful).

Definitions of centrality:

1. (Section 2): 
Defn(a): let B(x1,...,xk) = the band that contains all points (t,r) such that min xi(t) {\textless} y {\textless} max xi(t)
Defn(b): Let (Band depth{\_}j) BD{\_}n{\^{}}(j)(x) = proportion of number of bands with j functions that contain x.
Defn(c): Let (Band depth{\_}J) BD{\_}n,J(x) = sum{\_}{\{}j =1{\}}{\^{}}J BD{\_}n{\^{}}(j)(x)

* Proposed only using J = 3 for computation reasons* 
{\#}{\#} Aside: does this mirror Geenen's triangular inequality?


2. (Section 5: supposed to be better)
basically does the above, but changes BD{\_}n{\^{}}(j)(x) to MBD{\_}n{\^{}}(j)(x), where the later sums the proportion of time a function is contained in each specific band -this is easy to do with functions defined relative to finite steps.

suggests that BD focuses more on similarity of structure of curves whereas the MBD focuses less on this.

Other comments:
(Section 3.1) Claims this depth meets all properties from Zuo {\&} Serfling except affine invariance (and suggests this is not a natural requirement for functional depth)


FUTURE WORK:
1. This could be compared with the distance based depth (Geenen): specific - how do these bands of 3 curves relate to the triangle inequality structure proposed by Greenen? And how would it relative to just 2 curves?
2. Think about global depth and centrality in the lense of robustness? How to think about local depth? Vs density estimates?
3. follow up on Cuevas, Febrero, and Fraiman's ‘Robust Estimation and Classification for Functional Data via Projection-Based Depth Notions' - as it relates to the density vs depth debate},
author = {L$\backslash$'opez-Pintado, Sara and Romo, Juan},
doi = {10.1198/jasa.2009.0108},
file = {:Users/benjaminleroy/Library/Application Support/Mendeley Desktop/Downloaded/Pez-Pintado, Romo - 2009 - On the concept of depth for functional data.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Data depth,Functional data,Rank test for functions,Rank test functions},
mendeley-tags = {Data depth,Functional data,Rank test functions},
month = {jun},
number = {486},
pages = {718--734},
title = {{On the Concept of Depth for Functional Data}},
volume = {104},
year = {2009}
}
@article{King2016,
abstract = {Partially observed Markov process (POMP) models, also known as hidden Markov models or state space models, are ubiquitous tools for time series analysis. The R package pomp provides a very flexible framework for Monte Carlo statistical investigations using nonlinear, non-Gaussian POMP models. A range of modern statistical methods for POMP models have been implemented in this framework including sequential Monte Carlo, iterated filtering, particle Markov chain Monte Carlo, approximate Bayesian computation, maximum synthetic likelihood estimation, nonlinear forecasting, and trajectory matching. In this paper, we demonstrate the application of these methodologies using some simple toy problems. We also illustrate the specification of more complex POMP models, using a nonlinear epidemiological model with a discrete population, seasonality, and extra-demographic stochasticity. We discuss the specification of user-defined models and the development of additional methods within the programming environment provided by pomp.},
archivePrefix = {arXiv},
arxivId = {1509.00503},
author = {King, Aaron A. and Nguyen, Dao and Ionides, Edward L.},
doi = {10.18637/jss.v069.i12},
eprint = {1509.00503},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/pomp.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Hidden markov model,Markov processes,Maximum likelihood,Mechanistic model,Plug-and-play,R,Sequential Monte Carlo,State space model,Stochastic dynamical system,Time series},
number = {12},
pages = {1--43},
title = {{Statistical inference for partially observed markov processes via the R package pomp}},
volume = {69},
year = {2016}
}
@article{Bongiorno2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.04308v2},
author = {Bongiorno, Enea G and Goia, Aldo and Orientale, Piemonte},
eprint = {arXiv:1501.04308v2},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/depth/Some Insights About the Small Ball Probability Factorization for Hilbert Random Elements.pdf:pdf},
keywords = {hilbert functional data,karhunen,lo,small ball probability},
pages = {1--27},
title = {{Some Insights About the Small Ball Probability Factorization for Hilbert Random Elements arXiv : 1501 . 04308v2 [ math . PR ] 29 Mar 2016}},
year = {2016}
}
@article{Jenness2018,
abstract = {Package EpiModel provides tools for building, simulating, and analyzing mathematical models for the population dynamics of infectious disease transmission in R. Several classes of models are included, but the unique contribution of this software package is a general stochastic framework for modeling the spread of epidemics on networks. EpiModel integrates recent advances in statistical methods for network analysis (temporal exponential random graph models) that allow the epidemic modeling to be grounded in empirical data on contacts that can spread infection. This article provides an overview of both the modeling tools built into EpiModel, designed to facilitate learning for students new to modeling, and the application programming interface for extending package EpiModel, designed to facilitate the exploration of novel research questions for advanced modelers.},
author = {Jenness, Samuel M and Goodreau, Steven M and Morris, Martina},
doi = {10.18637/jss.v084.i08.EpiModel},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/EpiModel.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {R,epidemiology,infectious disease,mathematical model,networks},
number = {84},
title = {{EpiModel: An R Package for Mathematical Modeling of Infectious Disease over Networks}},
year = {2018}
}
@article{Gallagher2018,
abstract = {Agent-based models (ABMs) simulate interactions between autonomous agents in constrained environments over time and are often used for modeling the spread of infectious diseases. ABMs use information about agents and their environments as input, together referred to as a “synthetic ecosystem.” Previous approaches for generating synthetic ecosystems have some limitations: they are not open-source, cannot be adapted to new or updated input data sources, or do not allow for alternative methods for sampling agent characteristics and locations. We introduce a general framework for generating synthetic ecosystems, called “Synthetic Populations and Ecosystems of the World” (SPEW). SPEW lets researchers choose from a variety of sampling methods for agent characteristics and locations and is implemented as an open-source R package. We analyze the accuracy and computational efficiency of SPEW, given different sampling methods for agent characteristics and locations, and provide a suite of statistical and graphical tools to screen our generated ecosystems. SPEW has generated over five billion human agents across approximately 100,000 geographic regions in over 70 countries available online.},
archivePrefix = {arXiv},
arxivId = {1701.02383},
author = {Gallagher, Shannon and Richardson, Lee F. and Ventura, Samuel L. and Eddy, William F.},
doi = {10.1080/10618600.2018.1442342},
eprint = {1701.02383},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/timeternR/SPEW- Synthetic Populations and Ecosystems of the World.pdf:pdf},
issn = {15372715},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Agent-based models,Disease modeling,Iterative proportional fitting,Parallel computing,R,Synthetic data},
number = {4},
pages = {773--784},
publisher = {Taylor {\&} Francis},
title = {{SPEW: Synthetic Populations and Ecosystems of the World}},
url = {https://doi.org/10.1080/10618600.2018.1442342},
volume = {27},
year = {2018}
}
@article{Agostinelli2011,
abstract = {Local depth is a generalization of ordinary depth able to reveal local features of the probability distribution. Liu's simplicial depth is primarily used, but results for Tukey's halfspace depth are also derived. It is shown that the maximizers of local depth can help to detect the mode(s) of a probability distribution. This work is devoted to the univariate case, but the main definitions are stated in the general multivariate case. Theoretical results and applications are illustrated with several examples. {\textcopyright} 2010 Elsevier B.V.},
annote = {This paper proposes 2 local depth that switches the definitions of the global depths they update in such a ways the the geometry changes are pretty clean.

Local depth definitions:
1. Local simplicial depth (t is tuning): 
ld{\_}s(x, t) = Proportion of simplicials of size p that contain x (p should be at least d + 1 - where d is size of space) *is the extension of this actually check the triangle inequality and see if the max distance is below tau?*

2. Local halfspace depth:
d{\_}hs: halfspace is converted into slabs. then same definition is applied

In 4.1 they have some examples but also discuss some quick ways one might tune tau (specifically using the distribution of |X{\_}1 - X{\_}2| (in 1d space)

- example (22) shows that - with too small a selection of tau, you gets a high number of patial maximum ("local") - think about how to avoid it?

- proposed examining |x{\_}1 - x{\_}2| distribution (For 1d) to select tau.
* could we provide analysis to how tau is "just right"? (min should probably be our $\backslash$delta

- also mentioned a way to test if a point is a mode (this is depend on tau {\&} it seems to have the reverse null hypothesis then as desired) - not sure this is need to be extended -but if there was a way to think about H{\_}t,n as a function of tau we might be able to better capture tau. 

- I don't think tau really needs to go to zero as n-{\textgreater} infinite. Does this match with KDEs?},
author = {Agostinelli, Claudio and Romanazzi, Mario},
doi = {10.1016/j.jspi.2010.08.001},
file = {:Users/benjaminleroy/Library/Application Support/Mendeley Desktop/Downloaded/Agostinelli, Romanazzi - 2011 - Local depth.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Data depth,Halfspace depth,Multimodal distribution,Simplicial depth},
month = {feb},
number = {2},
pages = {817--830},
publisher = {Elsevier B.V.},
title = {{Local depth}},
volume = {141},
year = {2011}
}
