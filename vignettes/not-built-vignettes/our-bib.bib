@misc{epirecipes,
  title = {epirecipes: Epicookbook.},
  author = {Frost, Simon},
  howpublished = {\url{http://epirecip.es/epicookbook/chapters/ob18/c2/r}},
  note = {Accessed: 2020-08-31}
  }

@article{kermack1927,
	author = {Kermack, W. O. and McKendrick, A. G.},
	title = {A Contribution to the Mathematical Theory of Epidemics},
	volume = {115},
	number = {772},
	pages = {700--721},
	year = {1927},
	doi = {10.1098/rspa.1927.0118},
	publisher = {The Royal Society},
	issn = {0950-1207},
	URL = {http://rspa.royalsocietypublishing.org/content/115/772/700},
	eprint = {http://rspa.royalsocietypublishing.org/content/115/772/700.full.pdf},
	journal = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences}
}


@Book{anderson1992,
  author = 	 {Anderson, R and May, R.},
  title = 	 {Infectious Diseases of Humans},
  publisher = 	 {Oxford:  Oxford University Press},
  year = 	 {1992}
}

@inproceedings{getz2016,
  title={An agent-based model of school closing in under-vacccinated communities during measles outbreaks},
  author={Getz, Wayne M and Carlson, Colin and Dougherty, Eric and Porco, Travis C and Salter, Richard},
  booktitle={Proceedings of the Agent-Directed Simulation Symposium},
  pages={10},
  year={2016},
  organization={Society for Computer Simulation International}
}

@misc{cdc-measles2018,
  author = 	 {{Centers for Disease Control and Prevention}},
  title = 	 {Measles History},
  howpublished = {Available online at \url{https://www.cdc.gov/measles/about}},
  month = 	 {August},
  year = 	 {2018},
  OPTnote = 	 {},
  OPTannote = 	 {}
}


@article{cori2013,
    author = {Cori, Anne and Ferguson, Neil M. and Fraser, Christophe and Cauchemez, Simon},
    title = "{A New Framework and Software to Estimate Time-Varying Reproduction Numbers During Epidemics}",
    journal = {American Journal of Epidemiology},
    volume = {178},
    number = {9},
    pages = {1505-1512},
    year = {2013},
    month = {09},
    abstract = "{The quantification of transmissibility during epidemics is essential to designing and adjusting public health responses. Transmissibility can be measured by the reproduction number R, the average number of secondary cases caused by an infected individual. Several methods have been proposed to estimate R over the course of an epidemic; however, they are usually difficult to implement for people without a strong background in statistical modeling. Here, we present a ready-to-use tool for estimating R from incidence time series, which is implemented in popular software including Microsoft Excel (Microsoft Corporation, Redmond, Washington). This tool produces novel, statistically robust analytical estimates of R and incorporates uncertainty in the distribution of the serial interval (the time between the onset of symptoms in a primary case and the onset of symptoms in secondary cases). We applied the method to 5 historical outbreaks; the resulting estimates of R are consistent with those presented in the literature. This tool should help epidemiologists quantify temporal changes in the transmission intensity of future epidemics by using surveillance data.}",
    issn = {0002-9262},
    doi = {10.1093/aje/kwt133},
    url = {https://doi.org/10.1093/aje/kwt133},
    eprint = {https://academic.oup.com/aje/article-pdf/178/9/1505/17341195/kwt133.pdf},
}




@article{britton2011,
author = {Britton, Tom and Kypraios, Theodore and O'Neill, Philip D.},
title = {Inference for Epidemics with Three Levels of Mixing: Methodology and Application to a Measles Outbreak},
journal = {Scandinavian Journal of Statistics},
volume = {38},
number = {3},
pages = {578-599},
keywords = {Bayesian inference, epidemic model, Hagelloch, infectious disease data, Markov chain Monte Carlo, measles},
doi = {10.1111/j.1467-9469.2010.00726.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2010.00726.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9469.2010.00726.x},
abstract = {Abstract.  A stochastic epidemic model is defined in which each individual belongs to a household, a secondary grouping (typically school or workplace) and also the community as a whole. Moreover, infectious contacts take place in these three settings according to potentially different rates. For this model, we consider how different kinds of data can be used to estimate the infection rate parameters with a view to understanding what can and cannot be inferred. Among other things we find that temporal data can be of considerable inferential benefit compared with final size data, that the degree of heterogeneity in the data can have a considerable effect on inference for non-household transmission, and that inferences can be materially different from those obtained from a model with only two levels of mixing. We illustrate our findings by analysing a highly detailed dataset concerning a measles outbreak in Hagelloch, Germany.},
year = {2011}
}


@article{groendyke2012,
author = {Groendyke, Chris and Welch, David and Hunter, David R.},
title = {A Network-based Analysis of the 1861 Hagelloch Measles Data},
journal = {Biometrics},
volume = {68},
number = {3},
pages = {755-765},
keywords = {Exponential family Random Graph Model, Hagelloch, Measles, Networks},
doi = {10.1111/j.1541-0420.2012.01748.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2012.01748.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2012.01748.x},
abstract = {Summary In this article, we demonstrate a statistical method for fitting the parameters of a sophisticated network and epidemic model to disease data. The pattern of contacts between hosts is described by a class of dyadic independence exponential-family random graph models (ERGMs), whereas the transmission process that runs over the network is modeled as a stochastic susceptible-exposed-infectious-removed (SEIR) epidemic. We fit these models to very detailed data from the 1861 measles outbreak in Hagelloch, Germany. The network models include parameters for all recorded host covariates including age, sex, household, and classroom membership and household location whereas the SEIR epidemic model has exponentially distributed transmission times with gamma-distributed latent and infective periods. This approach allows us to make meaningful statements about the structure of the population—separate from the transmission process—as well as to provide estimates of various biological quantities of interest, such as the effective reproductive number, R. Using reversible jump Markov chain Monte Carlo, we produce samples from the joint posterior distribution of all the parameters of this model—the network, transmission tree, network parameters, and SEIR parameters—and perform Bayesian model selection to find the best-fitting network model. We compare our results with those of previous analyses and show that the ERGM network model better fits the data than a Bernoulli network model previously used. We also provide a software package, written in R, that performs this type of analysis.},
year = {2012}
}



@article{roberts2004,
author = {Neal, Peter J. and Roberts, Gareth O.},
title = {{Statistical inference and model selection for the 1861 Hagelloch measles epidemic}},
journal = {Biostatistics},
volume = {5},
number = {2},
pages = {249-261},
year = {2004},
doi = {10.1093/biostatistics/5.2.249},
URL = {http://dx.doi.org/10.1093/biostatistics/5.2.249},
eprint = {/oup/backfile/content_public/journal/biostatistics/5/2/10.1093/biostatistics/5.2.249/2/050249.pdf}
} 
 
  @Article{surveillance2017,
    author = {Sebastian Meyer and Leonhard Held and Michael Höhle},
    title = {Spatio-Temporal Analysis of Epidemic Phenomena Using the {R} Package {surveillance}},
    journal = {Journal of Statistical Software},
    year = {2017},
    volume = {77},
    number = {11},
    pages = {1--55},
    doi = {10.18637/jss.v077.i11},
  }



@misc{pfeilsticker1863,
  author = 	 {Pfeilsticker, A.},
  title = 	 {{Beiträge zur Pathologie der Masern mit besonderer Berücksichtigung der statistischen Verhältnisse}},
  school = 	 {{Eberhard-Karls-Universität Tübingen}},
  year = 	 {1863},
  url = {http://www.archive.org/details/beitrgezurpatho00pfeigoog}
}


@misc{oesterle1992,
  author = 	 {Oesterle, H.},
  title = 	 {{Statistische Reanalyse einer Masernepidemie 1861 in Hagelloch}},
  school = 	 {{Eberhard-Karls-Universität Tübingen}},
  year = 	 {1992}
}




 @Misc{covid19-kaggle-r,
    title = {Coronavirus COVID-19 (2019-nCoV) Epidemic Datasets},
    url = {https://www.kaggle.com/ds/574488},
    doi = {10.34740/KAGGLE/DS/574488},
    publisher = {Kaggle},
    author = {Emanuele Guidotti},
  }
  
  
@phdthesis{gallagher2019,
  title = {Catalyst: agents of change. Integration of compartment and agent-based models for use in infectious disease methodology},
  school = {Carnegie Mellon University},
  author = {Gallagher, Shannon K.},
  url = {https://skgallagher.github.io/papers/gallagher_dissertation.pdf},
  year = {2019}
}


@article{dong2020,
  title={An interactive web-based dashboard to track COVID-19 in real time},
  author={Dong, Ensheng and Du, Hongru and Gardner, Lauren},
  journal={The Lancet infectious diseases},
  year={2020},
  publisher={Elsevier}
}

@Manual{rcoronavirus2020,
    title = {coronavirus: The 2019 Novel Coronavirus COVID-19 (2019-nCoV) Dataset},
    author = {Rami Krispin},
    year = {2020},
    note = {R package version 0.1.0.9002},
    url = {https://github.com/covid19r/coronavirus},
  }


@article{Ciollaro2014,
abstract = {We introduce the functional mean-shift algorithm, an iterative algorithm for estimating the local modes of a surrogate density from functional data. We show that the algorithm can be used for cluster analysis of functional data. We propose a test based on the bootstrap for the significance of the estimated local modes of the surrogate density. We present two applications of our methodology. In the first application, we demonstrate how the functional mean-shift algorithm can be used to perform spike sorting, i.e. cluster neural activity curves. In the second application, we use the functional mean-shift algorithm to distinguish between original and fake signatures.},
archivePrefix = {arXiv},
arxivId = {1408.1187},
author = {Ciollaro, Mattia and Genovese, Christopher and Lei, Jing and Wasserman, Larry},
eprint = {1408.1187},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/functional{\_}psuedo{\_}density/The functional mean-shift algorithm for mode hunting and clustering in infinite dimensions.pdf:pdf},
number = {Figure 2},
title = {{The functional mean-shift algorithm for mode hunting and clustering in infinite dimensions}},
url = {http://arxiv.org/abs/1408.1187},
volume = {1},
year = {2014}
}


%How to cite item
%EpiModel: An R Package for Mathematical Modeling of Infectious Disease over Networks
%Citation Format  
@article{jeness2018-epimodel,
   author = {Samuel Jenness and Steven Goodreau and Martina Morris},
   title = {EpiModel: An R Package for Mathematical Modeling of Infectious Disease over Networks},
   journal = {Journal of Statistical Software, Articles},
   volume = {84},
   number = {8},
   year = {2018},
   keywords = {mathematical model; infectious disease; epidemiology; networks; R},
   abstract = {Package EpiModel provides tools for building, simulating, and analyzing mathematical models for the population dynamics of infectious disease transmission in R. Several classes of models are included, but the unique contribution of this software package is a general stochastic framework for modeling the spread of epidemics on networks. EpiModel integrates recent advances in statistical methods for network analysis (temporal exponential random graph models) that allow the epidemic modeling to be grounded in empirical data on contacts that can spread infection. This article provides an overview of both the modeling tools built into EpiModel, designed to facilitate learning for students new to modeling, and the application programming interface for extending package EpiModel, designed to facilitate the exploration of novel research questions for advanced modelers.},
   issn = {1548-7660},
   pages = {1--47},
   doi = {10.18637/jss.v084.i08},
   url = {https://www.jstatsoft.org/v084/i08}
}

@article{Agostinelli2011,
abstract = {Local depth is a generalization of ordinary depth able to reveal local features of the probability distribution. Liu's simplicial depth is primarily used, but results for Tukey's halfspace depth are also derived. It is shown that the maximizers of local depth can help to detect the mode(s) of a probability distribution. This work is devoted to the univariate case, but the main definitions are stated in the general multivariate case. Theoretical results and applications are illustrated with several examples. {\textcopyright} 2010 Elsevier B.V.},
annote = {This paper proposes 2 local depth that switches the definitions of the global depths they update in such a ways the the geometry changes are pretty clean.

Local depth definitions:
1. Local simplicial depth (t is tuning): 
ld{\_}s(x, t) = Proportion of simplicials of size p that contain x (p should be at least d + 1 - where d is size of space) *is the extension of this actually check the triangle inequality and see if the max distance is below tau?*

2. Local halfspace depth:
d{\_}hs: halfspace is converted into slabs. then same definition is applied

In 4.1 they have some examples but also discuss some quick ways one might tune tau (specifically using the distribution of |X{\_}1 - X{\_}2| (in 1d space)

- example (22) shows that - with too small a selection of tau, you gets a high number of patial maximum ("local") - think about how to avoid it?

- proposed examining |x{\_}1 - x{\_}2| distribution (For 1d) to select tau.
* could we provide analysis to how tau is "just right"? (min should probably be our $\backslash$delta

- also mentioned a way to test if a point is a mode (this is depend on tau {\&} it seems to have the reverse null hypothesis then as desired) - not sure this is need to be extended -but if there was a way to think about H{\_}t,n as a function of tau we might be able to better capture tau. 

- I don't think tau really needs to go to zero as n-{\textgreater} infinite. Does this match with KDEs?},
author = {Agostinelli, Claudio and Romanazzi, Mario},
doi = {10.1016/j.jspi.2010.08.001},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Data depth,Halfspace depth,Multimodal distribution,Simplicial depth},
month = {feb},
number = {2},
pages = {817--830},
publisher = {Elsevier B.V.},
title = {{Local depth}},
volume = {141},
year = {2011}
}

@misc{Geenens2017,
abstract = {The concept of depth has proved very important for multivariate and functional data analysis, as it essentially acts as a surrogate for the notion a ranking of observations which is absent in more than one dimension. Yet, naive attempts to extend to the functional context some succesful multivariate depth measures have been seen to lead to absurd results, as they failed to take into acount in their construction purely functional features such as continuity, smoothness and contiguity. In this paper we suggest a natural depth which satisfies all the properties that a valid functional depth measure should fulfill. It is explicitly constructed on a certain distance d between functional objects, which has to be selected by the analyst. This allows the depth measure to really be tailored to the data at hand and to the ultimate goal of the analysis, a very desirable property in functional data analysis given the polymorphic nature of those data. This flexibility is thoroughly illustrated by several real data analyses, including an original one pertaining to the field of Symbolic Data Analysis},
author = {Geenens, Gery and Nieto-Reyes, Alicia},
title = {{On the functional distance-based depth}},
year = {2017}
}

@article{Ferraty2012,
abstract = {A density function is generally not well defined in functional data context, but we can define a surrogate of a probability density, also called pseudo-density, when the small ball probability can be approximated by the product of two independent functions, one depending only on the centre of the ball. The aim of this paper is to study two kernel methods for estimating a surrogate probability density for functional data. We present asymptotic properties of these estimators: the convergence in probability and their rates. Simulations are given, including a functional version of smoother bootstrap selection of the parameters of the estimate. {\textcopyright} 2012 Copyright American Statistical Association and Taylor {\&} Francis.},
author = {Ferraty, Fr{\'{e}}d{\'{e}}ric and Kudraszow, Nadia and Vieu, Philippe},
doi = {10.1080/10485252.2012.671943},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/functional{\_}psuedo{\_}density/surrogate{\_}density{\_}online{\_}version.pdf:pdf},
issn = {10485252},
journal = {Journal of Nonparametric Statistics},
keywords = {functional data,k-nearest neighbour method,kernel estimators,small ball probability,smoother bootstrap},
number = {2},
pages = {447--464},
title = {{Nonparametric estimation of a surrogate density function in infinite-dimensional spaces}},
volume = {24},
year = {2012}
}

@article{king2010,
  title={pomp: Statistical inference for partially observed Markov processes (R package)},
  author={King, AA and Ionides, EL and Bret{\'o}, CM and Ellner, S and Kendall, B and Wearing, H and Ferrari, MJ and Lavine, M and Reuman, DC},
  journal={URL http://pomp. r-forge. r-rproject. org},
  year={2010}
}

@book{Vovk2005,
author = {Vovk, Vladimir and Gammerman, Alex and Shafer, Glenn},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/depth/2005{\_}Book{\_}AlgorithmicLearningInARandomWo.pdf:pdf},
isbn = {0-387-00152-2},
publisher = {Springer Science {\&} Business Media},
title = {{Algorithmic Learning in a Random World}},
year = {2005}
}

@article{Dalmasso2019,
abstract = {Hurricanes and, more generally, tropical cyclones (TCs) are rare, complex natural phenomena of both scientific and public interest. The importance of understanding TCs in a changing climate has increased as recent TCs have had devastating impacts on human lives and communities. Moreover, good prediction and understanding about the complex nature of TCs can mitigate some of these human and property losses. Though TCs have been studied from many different angles, more work is needed from a statistical approach of providing prediction regions. The current state-of-the-art in TC prediction bands comes from the National Hurricane Center of the National Oceanographic and Atmospheric Administration (NOAA), whose proprietary model provides "cones of uncertainty" for TCs through an analysis of historical forecast errors. The contribution of this paper is twofold. We introduce a new pipeline that encourages transparent and adaptable prediction band development by streamlining cyclone track simulation and prediction band generation. We also provide updates to existing models and novel statistical methodologies in both areas of the pipeline, respectively.},
archivePrefix = {arXiv},
arxivId = {1906.08832},
author = {Dalmasso, Niccol{\`{o}} and Dunn, Robin and LeRoy, Benjamin and Schafer, Chad},
eprint = {1906.08832},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/tropical{\_}cyclones/A Flexible Pipeline for Prediction of Tropical Cyclone Paths.pdf:pdf},
journal = {ICML Workshop: "Climate Change: How can AI Help?"},
title = {{A Flexible Pipeline for Prediction of Tropical Cyclone Paths}},
url = {http://arxiv.org/abs/1906.08832},
year = {2019}
}


@article{Izbicki2017,
abstract = {There is a growing demand for nonparametric conditional density estimators (CDEs) in fields such as astronomy and economics. In astronomy, for example, one can dramatically improve estimates of the parameters that dictate the evolution of the Universe by working with full conditional densities instead of regression (i.e., conditional mean) estimates. More generally, standard regression falls short in any prediction problem where the distribution of the response is more complex with multi-modality, asymmetry or heteroscedastic noise. Nevertheless, much of the work on high-dimensional inference concerns regression and classification only, whereas research on density estimation has lagged behind. Here we propose FlexCode, a fully nonparametric approach to conditional density estimation that reformulates CDE as a non-parametric orthogonal series problem where the expansion coefficients are estimated by regression. By taking such an approach, one can efficiently estimate conditional densities and not just expectations in high dimensions by drawing upon the success in high-dimensional regression. Depending on the choice of regression procedure, our method can adapt to a variety of challenging high-dimensional settings with different structures in the data (e.g., a large number of irrelevant components and nonlinear manifold structure) as well as different data types (e.g., functional data, mixed data types and sample sets). We study the theoretical and empirical performance of our proposed method, and we compare our approach with traditional conditional density estimators on simulated as well as real-world data, such as photometric galaxy data, Twitter data, and line-of-sight velocities in a galaxy cluster.},
archivePrefix = {arXiv},
arxivId = {1704.08095},
author = {Izbicki, Rafael and Lee, Ann B.},
doi = {10.1214/17-EJS1302},
eprint = {1704.08095},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/conformal prediction/Converting high-dimensional regression to high-dimensional conditional density estimation.pdf:pdf},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {62G07, 62G15, 62G08Nonparametric inference, condit},
number = {2},
pages = {2800--2831},
title = {{Converting high-dimensional regression to high-dimensional conditional density estimation}},
volume = {11},
year = {2017}
}

@article{Lei2013,
abstract = {This article introduces a new approach to prediction by bringing together two different nonparametric ideas: distribution-free inference and nonparametric smoothing. Specifically, we consider the problem of constructing nonparametric tolerance/prediction sets. We start from the general conformal prediction approach, and we use a kernel density estimator as a measure of agreement between a sample point and the underlying distribution. The resulting prediction set is shown to be closely related to plug-in density level sets with carefully chosen cutoff values. Under standard smoothness conditions, we get an asymptotic efficiency result that is near optimal for a wide range of function classes. But the coverage is guaranteed whether or not the smoothness conditions hold and regardless of the sample size. The performance of our method is investigated through simulation studies and illustrated in a real data example. {\textcopyright} 2013 American Statistical Association.},
archivePrefix = {arXiv},
arxivId = {1203.5422},
author = {Lei, Jing and Robins, James and Wasserman, Larry},
doi = {10.1080/01621459.2012.751873},
eprint = {1203.5422},
file = {:Users/benjaminleroy/Library/Application Support/Mendeley Desktop/Downloaded/Lei, Robins, Wasserman - 2013 - Distribution-free prediction sets.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Conformal prediction,Consistency,Density level sets,Finite sample,Kernel density},
number = {501},
pages = {278--287},
title = {{Distribution-free prediction sets}},
volume = {108},
year = {2013}
}

@article{Walther1997,
abstract = {A new method for smoothing a multivariate data set is introduced that is based on a simple geometric operation. This method is applied to the problem of estimating level sets of a density and minimum volume sets with given probability content. Building on existing techniques, the resulting estimator combines excellent theoretical and computational properties: It converges with the minimax rates (up to log factors) in most cases where these rates are known and, at the same time, it can be computed, visualized, stored and manipulated by simple algorithms and tools. It is applicable to a wide class of sets that is characterized explicitly in terms of the underlying densities and includes nonconvex and disconnected sets, and it is argued that it should give reasonable results in completely general situations. Applications to the construction of multivariate confidence regions in frequentist and Bayesian contexts are briefly mentioned.},
author = {Walther, Guenther},
doi = {10.1214/aos/1030741072},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/topology/GRANULOMETRIC SMOOTHING.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Excess mass,Granulometry,Highest posterior density regions,Level sets,Minimum volume sets,Mode,Multivariate confidence regions,Smoothing},
number = {6},
pages = {2273--2299},
title = {{Granulometric smoothing}},
volume = {25},
year = {1997}
}


@inproceedings{Jiang2017,
abstract = {We show that DBSCAN can estimate the connected components of the $\lambda$-density level set {\{}x: f{\{}x) ≥ $\lambda${\}} given n i.i.d. samples from an unknown density f. We characterize the regularity of the level set boundaries using parameter $\beta$ {\textgreater} 0 and analyze the estimation error under the Hausdorff metric When the data lies in ℝD we obtain a rate of {\~{O}}(n-1/(2$\beta$+D)), which matches known lower bounds up to logarithmic factors. When the data lies on an embedded unknown d-dimensional manifold inℝD, then we obtain a rate of {\~{O}}(n-1/(2$\beta$+dmax{\{}1, $\beta${\}}). Finally, we provide adaptive parameter tuning in order to attain these rates with no a priori knowledge of the intrinsic dimension, density, or $\beta$.}},
archivePrefix = {arXiv},
arxivId = {1703.03503},
author = {Jiang, Heinrich},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1703.03503},
file = {:Users/benjaminleroy/Library/Application Support/Mendeley Desktop/Downloaded/Jiang - 2017 - Density Level Set Estimation on Manifolds with DBSCAN.pdf:pdf},
isbn = {9781510855144},
month = {mar},
pages = {2643--2655},
title = {{Density level set estimation on manifolds with DBSCAN}},
url = {http://arxiv.org/abs/1703.03503},
volume = {4},
year = {2017}
}

@book{Grenander1981a,
address = {New York},
author = {Grenander, Ulf},
publisher = {Wiley-Interscience},
title = {{Abstract inference}},
year = {1981}
}

@article{Chiou2014,
abstract = {We propose an extended version of the classical Karhunen-L{\`{o}}eve expansion of a multivariate random process, termed a normalized multivariate functional principal component (mFPCn) representation. This takes variations between the components of the process into account and takes advantage of component dependencies through the pairwise cross-covariance functions. This approach leads to a single set of multivariate functional principal component scores, which serve well as a proxy for multivariate functional data. We derive the consistency properties for the estimates of the mFPCn, and the asymptotic distributions for statistical inferences. We illustrate the finite sample performance of this approach through the analysis of a traffic flow data set, including an application to clustering and a simulation study. The mFPCn approach serves as a basic and useful statistical tool for multivariate functional data analysis.},
author = {Chiou, Jeng Min and Chen, Yu Ting and Yang, Ya Fang},
doi = {10.5705/ss.2013.305},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/fda/MULTIVARIATE FUNCTIONAL PRINCIPAL COMPONENT ANALYSIS- A NORMALIZATION APPROACH.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {Karhunen-L{\`{o}}Eve expansion,Mercer's theorem,Multivariate functional data,Normalization,Traffic flow},
number = {4},
pages = {1571--1596},
title = {{Multivariate functional principal component analysis: A normalization approach}},
volume = {24},
year = {2014}
}

@article{Chen2012,
abstract = {For functional data lying on an unknown nonlinear low-dimensional space, we study manifold learning and introduce the notions of manifold mean, manifold modes of functional variation and of functional manifold components. These constitute nonlinear representations of functional data that complement classical linear representations such as eigenfunctions and functional principal components. Our manifold learning procedures borrow ideas from existing nonlinear dimension reduction methods, which we modify to address functional data settings. In simulations and applications, we study examples of functional data which lie on a manifold and validate the superior behavior of manifold mean and functional manifold components over traditional cross-sectional mean and functional principal components. We also include consistency proofs for our estimators under certain assumptions. {\textcopyright} Institute of Mathematical Statistics, 2012.},
author = {Chen, Dong and M{\"{u}}ller, Hans Georg},
doi = {10.1214/11-AOS936},
file = {:Users/benjaminleroy/Library/Application Support/Mendeley Desktop/Downloaded/Chen, M{\"{u}}ller - 2012 - Nonlinear manifold representations for functional data.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Dimension reduction,Functional data analysis,Functional manifold components,Modes of functional variation,Smoothing},
month = {feb},
number = {1},
pages = {1--29},
title = {{Nonlinear manifold representations for functional data}},
volume = {40},
year = {2012}
}

@article{Buchman2011,
abstract = {We present nonparametric techniques for constructing and verifying density estimates from high-dimensional data whose irregular dependence structure cannot be modelled by parametric multivariate distributions. A low-dimensional representation of the data is critical in such situations because of the curse of dimensionality. Our proposed methodology consists of three main parts: (1) data reparameterization via dimensionality reduction, wherein the data are mapped into a space where standard techniques can be used for density estimation and simulation; (2) inverse mapping, in which simulated points are mapped back to the high-dimensional input space; and (3) verification, in which the quality of the estimate is assessed by comparing simulated samples with the observed data. These approaches are illustrated via an exploration of the spatial variability of tropical cyclones in the North Atlantic; each datum in this case is an entire hurricane trajectory. We conclude the paper with a discussion of extending the methods to model the relationship between TC variability and climatic variables. {\textcopyright} 2009 Elsevier B.V.},
author = {Buchman, Susan M. and Lee, Ann B. and Schafer, Chad M.},
doi = {10.1016/j.stamet.2009.07.002},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/tropical{\_}cyclones/High-dimensional density estimation via SCA- An example in the modelling of hurricane tracks.pdf:pdf},
issn = {15723127},
journal = {Statistical Methodology},
keywords = {Application to physical sciences,Dimension reduction,Nonparametric density estimation},
number = {1},
pages = {18--30},
publisher = {Elsevier B.V.},
title = {{High-dimensional density estimation via SCA: An example in the modelling of hurricane tracks}},
url = {http://dx.doi.org/10.1016/j.stamet.2009.07.002},
volume = {8},
year = {2011}
}