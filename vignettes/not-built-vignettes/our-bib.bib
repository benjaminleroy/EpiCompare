@Book{anderson1992,
  author = 	 {Anderson, R and May, R.},
  title = 	 {Infectious Diseases of Humans},
  publisher = 	 {Oxford:  Oxford University Press},
  year = 	 {1992}
}

@inproceedings{getz2016,
  title={An agent-based model of school closing in under-vacccinated communities during measles outbreaks},
  author={Getz, Wayne M and Carlson, Colin and Dougherty, Eric and Porco, Travis C and Salter, Richard},
  booktitle={Proceedings of the Agent-Directed Simulation Symposium},
  pages={10},
  year={2016},
  organization={Society for Computer Simulation International}
}

@misc{cdc-measles2018,
  author = 	 {{Centers for Disease Control and Prevention}},
  title = 	 {Measles History},
  howpublished = {Available online at \url{https://www.cdc.gov/measles/about}},
  month = 	 {August},
  year = 	 {2018},
  OPTnote = 	 {},
  OPTannote = 	 {}
}


@article{cori2013,
    author = {Cori, Anne and Ferguson, Neil M. and Fraser, Christophe and Cauchemez, Simon},
    title = "{A New Framework and Software to Estimate Time-Varying Reproduction Numbers During Epidemics}",
    journal = {American Journal of Epidemiology},
    volume = {178},
    number = {9},
    pages = {1505-1512},
    year = {2013},
    month = {09},
    abstract = "{The quantification of transmissibility during epidemics is essential to designing and adjusting public health responses. Transmissibility can be measured by the reproduction number R, the average number of secondary cases caused by an infected individual. Several methods have been proposed to estimate R over the course of an epidemic; however, they are usually difficult to implement for people without a strong background in statistical modeling. Here, we present a ready-to-use tool for estimating R from incidence time series, which is implemented in popular software including Microsoft Excel (Microsoft Corporation, Redmond, Washington). This tool produces novel, statistically robust analytical estimates of R and incorporates uncertainty in the distribution of the serial interval (the time between the onset of symptoms in a primary case and the onset of symptoms in secondary cases). We applied the method to 5 historical outbreaks; the resulting estimates of R are consistent with those presented in the literature. This tool should help epidemiologists quantify temporal changes in the transmission intensity of future epidemics by using surveillance data.}",
    issn = {0002-9262},
    doi = {10.1093/aje/kwt133},
    url = {https://doi.org/10.1093/aje/kwt133},
    eprint = {https://academic.oup.com/aje/article-pdf/178/9/1505/17341195/kwt133.pdf},
}




@article{britton2011,
author = {Britton, Tom and Kypraios, Theodore and O'Neill, Philip D.},
title = {Inference for Epidemics with Three Levels of Mixing: Methodology and Application to a Measles Outbreak},
journal = {Scandinavian Journal of Statistics},
volume = {38},
number = {3},
pages = {578-599},
keywords = {Bayesian inference, epidemic model, Hagelloch, infectious disease data, Markov chain Monte Carlo, measles},
doi = {10.1111/j.1467-9469.2010.00726.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2010.00726.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9469.2010.00726.x},
abstract = {Abstract.  A stochastic epidemic model is defined in which each individual belongs to a household, a secondary grouping (typically school or workplace) and also the community as a whole. Moreover, infectious contacts take place in these three settings according to potentially different rates. For this model, we consider how different kinds of data can be used to estimate the infection rate parameters with a view to understanding what can and cannot be inferred. Among other things we find that temporal data can be of considerable inferential benefit compared with final size data, that the degree of heterogeneity in the data can have a considerable effect on inference for non-household transmission, and that inferences can be materially different from those obtained from a model with only two levels of mixing. We illustrate our findings by analysing a highly detailed dataset concerning a measles outbreak in Hagelloch, Germany.},
year = {2011}
}


@article{groendyke2012,
author = {Groendyke, Chris and Welch, David and Hunter, David R.},
title = {A Network-based Analysis of the 1861 Hagelloch Measles Data},
journal = {Biometrics},
volume = {68},
number = {3},
pages = {755-765},
keywords = {Exponential family Random Graph Model, Hagelloch, Measles, Networks},
doi = {10.1111/j.1541-0420.2012.01748.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2012.01748.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2012.01748.x},
abstract = {Summary In this article, we demonstrate a statistical method for fitting the parameters of a sophisticated network and epidemic model to disease data. The pattern of contacts between hosts is described by a class of dyadic independence exponential-family random graph models (ERGMs), whereas the transmission process that runs over the network is modeled as a stochastic susceptible-exposed-infectious-removed (SEIR) epidemic. We fit these models to very detailed data from the 1861 measles outbreak in Hagelloch, Germany. The network models include parameters for all recorded host covariates including age, sex, household, and classroom membership and household location whereas the SEIR epidemic model has exponentially distributed transmission times with gamma-distributed latent and infective periods. This approach allows us to make meaningful statements about the structure of the population—separate from the transmission process—as well as to provide estimates of various biological quantities of interest, such as the effective reproductive number, R. Using reversible jump Markov chain Monte Carlo, we produce samples from the joint posterior distribution of all the parameters of this model—the network, transmission tree, network parameters, and SEIR parameters—and perform Bayesian model selection to find the best-fitting network model. We compare our results with those of previous analyses and show that the ERGM network model better fits the data than a Bernoulli network model previously used. We also provide a software package, written in R, that performs this type of analysis.},
year = {2012}
}



@article{roberts2004,
author = {Neal, Peter J. and Roberts, Gareth O.},
title = {{Statistical inference and model selection for the 1861 Hagelloch measles epidemic}},
journal = {Biostatistics},
volume = {5},
number = {2},
pages = {249-261},
year = {2004},
doi = {10.1093/biostatistics/5.2.249},
URL = {http://dx.doi.org/10.1093/biostatistics/5.2.249},
eprint = {/oup/backfile/content_public/journal/biostatistics/5/2/10.1093/biostatistics/5.2.249/2/050249.pdf}
} 
 
  @Article{surveillance2017,
    author = {Sebastian Meyer and Leonhard Held and Michael Höhle},
    title = {Spatio-Temporal Analysis of Epidemic Phenomena Using the {R} Package {surveillance}},
    journal = {Journal of Statistical Software},
    year = {2017},
    volume = {77},
    number = {11},
    pages = {1--55},
    doi = {10.18637/jss.v077.i11},
  }



@misc{pfeilsticker1863,
  author = 	 {Pfeilsticker, A.},
  title = 	 {{Beiträge zur Pathologie der Masern mit besonderer Berücksichtigung der statistischen Verhältnisse}},
  school = 	 {{Eberhard-Karls-Universität Tübingen}},
  year = 	 {1863},
  url = {http://www.archive.org/details/beitrgezurpatho00pfeigoog}
}


@misc{oesterle1992,
  author = 	 {Oesterle, H.},
  title = 	 {{Statistische Reanalyse einer Masernepidemie 1861 in Hagelloch}},
  school = 	 {{Eberhard-Karls-Universität Tübingen}},
  year = 	 {1992}
}




 @Misc{covid19-kaggle-r,
    title = {Coronavirus COVID-19 (2019-nCoV) Epidemic Datasets},
    url = {https://www.kaggle.com/ds/574488},
    doi = {10.34740/KAGGLE/DS/574488},
    publisher = {Kaggle},
    author = {Emanuele Guidotti},
  }
  
  
@phdthesis{gallagher2019,
  title = {Catalyst: agents of change. Integration of compartment and agent-based models for use in infectious disease methodology},
  school = {Carnegie Mellon University},
  author = {Gallagher, Shannon K.},
  url = {https://skgallagher.github.io/papers/gallagher_dissertation.pdf},
  year = {2019}
}


@article{dong2020,
  title={An interactive web-based dashboard to track COVID-19 in real time},
  author={Dong, Ensheng and Du, Hongru and Gardner, Lauren},
  journal={The Lancet infectious diseases},
  year={2020},
  publisher={Elsevier}
}

@Manual{rcoronavirus2020,
    title = {coronavirus: The 2019 Novel Coronavirus COVID-19 (2019-nCoV) Dataset},
    author = {Rami Krispin},
    year = {2020},
    note = {R package version 0.1.0.9002},
    url = {https://github.com/covid19r/coronavirus},
  }


@article{Ciollaro2014,
abstract = {We introduce the functional mean-shift algorithm, an iterative algorithm for estimating the local modes of a surrogate density from functional data. We show that the algorithm can be used for cluster analysis of functional data. We propose a test based on the bootstrap for the significance of the estimated local modes of the surrogate density. We present two applications of our methodology. In the first application, we demonstrate how the functional mean-shift algorithm can be used to perform spike sorting, i.e. cluster neural activity curves. In the second application, we use the functional mean-shift algorithm to distinguish between original and fake signatures.},
archivePrefix = {arXiv},
arxivId = {1408.1187},
author = {Ciollaro, Mattia and Genovese, Christopher and Lei, Jing and Wasserman, Larry},
eprint = {1408.1187},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/functional{\_}psuedo{\_}density/The functional mean-shift algorithm for mode hunting and clustering in infinite dimensions.pdf:pdf},
number = {Figure 2},
title = {{The functional mean-shift algorithm for mode hunting and clustering in infinite dimensions}},
url = {http://arxiv.org/abs/1408.1187},
volume = {1},
year = {2014}
}


%How to cite item
%EpiModel: An R Package for Mathematical Modeling of Infectious Disease over Networks
%Citation Format  
@article{jeness2018-epimodel,
   author = {Samuel Jenness and Steven Goodreau and Martina Morris},
   title = {EpiModel: An R Package for Mathematical Modeling of Infectious Disease over Networks},
   journal = {Journal of Statistical Software, Articles},
   volume = {84},
   number = {8},
   year = {2018},
   keywords = {mathematical model; infectious disease; epidemiology; networks; R},
   abstract = {Package EpiModel provides tools for building, simulating, and analyzing mathematical models for the population dynamics of infectious disease transmission in R. Several classes of models are included, but the unique contribution of this software package is a general stochastic framework for modeling the spread of epidemics on networks. EpiModel integrates recent advances in statistical methods for network analysis (temporal exponential random graph models) that allow the epidemic modeling to be grounded in empirical data on contacts that can spread infection. This article provides an overview of both the modeling tools built into EpiModel, designed to facilitate learning for students new to modeling, and the application programming interface for extending package EpiModel, designed to facilitate the exploration of novel research questions for advanced modelers.},
   issn = {1548-7660},
   pages = {1--47},
   doi = {10.18637/jss.v084.i08},
   url = {https://www.jstatsoft.org/v084/i08}
}

@article{Agostinelli2011,
abstract = {Local depth is a generalization of ordinary depth able to reveal local features of the probability distribution. Liu's simplicial depth is primarily used, but results for Tukey's halfspace depth are also derived. It is shown that the maximizers of local depth can help to detect the mode(s) of a probability distribution. This work is devoted to the univariate case, but the main definitions are stated in the general multivariate case. Theoretical results and applications are illustrated with several examples. {\textcopyright} 2010 Elsevier B.V.},
annote = {This paper proposes 2 local depth that switches the definitions of the global depths they update in such a ways the the geometry changes are pretty clean.

Local depth definitions:
1. Local simplicial depth (t is tuning): 
ld{\_}s(x, t) = Proportion of simplicials of size p that contain x (p should be at least d + 1 - where d is size of space) *is the extension of this actually check the triangle inequality and see if the max distance is below tau?*

2. Local halfspace depth:
d{\_}hs: halfspace is converted into slabs. then same definition is applied

In 4.1 they have some examples but also discuss some quick ways one might tune tau (specifically using the distribution of |X{\_}1 - X{\_}2| (in 1d space)

- example (22) shows that - with too small a selection of tau, you gets a high number of patial maximum ("local") - think about how to avoid it?

- proposed examining |x{\_}1 - x{\_}2| distribution (For 1d) to select tau.
* could we provide analysis to how tau is "just right"? (min should probably be our $\backslash$delta

- also mentioned a way to test if a point is a mode (this is depend on tau {\&} it seems to have the reverse null hypothesis then as desired) - not sure this is need to be extended -but if there was a way to think about H{\_}t,n as a function of tau we might be able to better capture tau. 

- I don't think tau really needs to go to zero as n-{\textgreater} infinite. Does this match with KDEs?},
author = {Agostinelli, Claudio and Romanazzi, Mario},
doi = {10.1016/j.jspi.2010.08.001},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Data depth,Halfspace depth,Multimodal distribution,Simplicial depth},
month = {feb},
number = {2},
pages = {817--830},
publisher = {Elsevier B.V.},
title = {{Local depth}},
volume = {141},
year = {2011}
}

@misc{Geenens2017,
abstract = {The concept of depth has proved very important for multivariate and functional data analysis, as it essentially acts as a surrogate for the notion a ranking of observations which is absent in more than one dimension. Yet, naive attempts to extend to the functional context some succesful multivariate depth measures have been seen to lead to absurd results, as they failed to take into acount in their construction purely functional features such as continuity, smoothness and contiguity. In this paper we suggest a natural depth which satisfies all the properties that a valid functional depth measure should fulfill. It is explicitly constructed on a certain distance d between functional objects, which has to be selected by the analyst. This allows the depth measure to really be tailored to the data at hand and to the ultimate goal of the analysis, a very desirable property in functional data analysis given the polymorphic nature of those data. This flexibility is thoroughly illustrated by several real data analyses, including an original one pertaining to the field of Symbolic Data Analysis},
author = {Geenens, Gery and Nieto-Reyes, Alicia},
title = {{On the functional distance-based depth}},
year = {2017}
}

@article{Ferraty2012,
abstract = {A density function is generally not well defined in functional data context, but we can define a surrogate of a probability density, also called pseudo-density, when the small ball probability can be approximated by the product of two independent functions, one depending only on the centre of the ball. The aim of this paper is to study two kernel methods for estimating a surrogate probability density for functional data. We present asymptotic properties of these estimators: the convergence in probability and their rates. Simulations are given, including a functional version of smoother bootstrap selection of the parameters of the estimate. {\textcopyright} 2012 Copyright American Statistical Association and Taylor {\&} Francis.},
author = {Ferraty, Fr{\'{e}}d{\'{e}}ric and Kudraszow, Nadia and Vieu, Philippe},
doi = {10.1080/10485252.2012.671943},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/functional{\_}psuedo{\_}density/surrogate{\_}density{\_}online{\_}version.pdf:pdf},
issn = {10485252},
journal = {Journal of Nonparametric Statistics},
keywords = {functional data,k-nearest neighbour method,kernel estimators,small ball probability,smoother bootstrap},
number = {2},
pages = {447--464},
title = {{Nonparametric estimation of a surrogate density function in infinite-dimensional spaces}},
volume = {24},
year = {2012}
}

@article{king2010,
  title={pomp: Statistical inference for partially observed Markov processes (R package)},
  author={King, AA and Ionides, EL and Bret{\'o}, CM and Ellner, S and Kendall, B and Wearing, H and Ferrari, MJ and Lavine, M and Reuman, DC},
  journal={URL http://pomp. r-forge. r-rproject. org},
  year={2010}
}

@book{Vovk2005,
author = {Vovk, Vladimir and Gammerman, Alex and Shafer, Glenn},
file = {:Users/benjaminleroy/Documents/CMU/research{\_}reading/depth/2005{\_}Book{\_}AlgorithmicLearningInARandomWo.pdf:pdf},
isbn = {0-387-00152-2},
publisher = {Springer Science {\&} Business Media},
title = {{Algorithmic Learning in a Random World}},
year = {2005}
}